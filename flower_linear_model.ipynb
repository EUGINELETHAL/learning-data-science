{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Deep Neural Network (DNN) model to discern Iris flowers\n",
    "We'll build a DNN model to predict the type of Iris flower species given only a flower's natural features(sepal and petal length).\n",
    "\n",
    "We will use Google's Tensorflow: an open source machine learning tool for everyone.\n",
    "\n",
    "The types of flowers we would like to distunguish is Iris Versicolour, Iris Virginica, Iris Setosa. \n",
    "\n",
    "The flowers look like this:\n",
    "\n",
    "* Iris Virginica\n",
    "![Iris Virginica](images/178px-Iris_virginica.jpg)\n",
    "\n",
    "* Iris Versicolor\n",
    "![Iris_versicolor](images/193px-Iris_versicolor_3.jpg)\n",
    "\n",
    "* Iris Setosa\n",
    "![Iris Kosaciec](images/109px-Kosaciec_szczecinkowaty_Iris_setosa.jpg)\n",
    "\n",
    "The data columns we have are:\n",
    "* sepal length in cm\n",
    "* sepal width in cm\n",
    "* petal length in cm\n",
    "* petal width in cm\n",
    "\n",
    "These data points will be used to train the model and test its accuracy.\n",
    "\n",
    "Time to write some Neural Nets!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n",
      "       0    1    2    3  4\n",
      "40   5.2  3.5  1.5  0.2  0\n",
      "31   6.7  3.3  5.7  2.5  2\n",
      "134  6.2  2.2  4.5  1.5  1\n",
      "1    5.1  3.5  1.4  0.2  0\n",
      "147  6.7  2.5  5.8  1.8  2\n",
      "[120, 4, 'setosa', 'versicolor', 'virginica']\n",
      "[30, 4, 'setosa', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: split the data into training and testing data\n",
    "# Key points to note:\n",
    "############ In the data frame that we wish to form,\n",
    "############ * Setosa's value = 0, Versicolor = 1 and Virginica = 2\n",
    "########### * The species column will be changed to contain these values before split the data.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read data from the csv file\n",
    "actual_data = pd.read_csv(\"iris.csv\")\n",
    "data = pd.read_csv(\"iris.csv\", skiprows=[0], header=None)\n",
    "labels_data = pd.read_csv(\"iris.csv\", usecols=[4], skiprows=[0], header=None)\n",
    "labels = np.unique(np.array(labels_data, 'str'))\n",
    "\n",
    "print (labels)\n",
    "\n",
    "# iterate through each row, changing the last column (4) to have integers instead of flower names\n",
    "for index, row in data.iterrows():\n",
    "    if row[4] == labels[0]:\n",
    "        data.loc[index, 4] = 0\n",
    "    elif row[4] == labels[1]:\n",
    "        data.loc[index, 4] = 1\n",
    "    else:\n",
    "        data.loc[index, 4] = 2\n",
    "\n",
    "# shuffle the newly formatted data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "y = actual_data.species\n",
    "\n",
    "# split that data, 80-20 rule\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2)\n",
    "print (X_test.head())\n",
    "# write the split data to csv\n",
    "# but first create a custom header for the training and test csv files\n",
    "# and omit the species column from the header since that's what we want to predict (hence the \"- 1\")\n",
    "X_train_header = list((len(X_train.index), len(X_train.columns) - 1))  + list(labels)\n",
    "X_test_header = list((len(X_test.index), len(X_test.columns) - 1)) + list(labels)\n",
    "print (X_train_header)\n",
    "print (X_test_header)\n",
    "X_train.to_csv(\"iris_training.csv\", index=False, index_label=False, header=X_train_header)\n",
    "X_test.to_csv(\"iris_test.csv\", index=False, index_label=False, header=X_test_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why we had to format the header of the training and test csv to be \n",
    "(row_lenth, column_length, 'setosa', 'versicolor', 'virginica') is to \n",
    "make the `load_csv_with_header` function be able to read our data. Data preparation matters. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0 2 2 1 1 2 2 1 1 2 2 2 1 1 1 1 1 0 1 1 0 1 0 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "\n",
    "# Data files containing our sepal and petal features\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "\n",
    "training_set = base.load_csv_with_header(\n",
    "    filename=IRIS_TRAINING, \n",
    "    features_dtype=np.float32, \n",
    "    target_dtype=np.int)\n",
    "\n",
    "test_set = base.load_csv_with_header(\n",
    "    filename=IRIS_TEST,\n",
    "    features_dtype=np.float32,\n",
    "    target_dtype=np.int)\n",
    "\n",
    "print(test_set.target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify that all feature columns have real-value data\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we build the 3 layer Deep Nueral Network classfier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c2062ff98>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/iris_model'}\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.contrib.learn.DNNClassifier(\n",
    "    feature_columns=featured_columns,\n",
    "    hidden_units=[10, 20, 10],\n",
    "    n_classes=3,\n",
    "    model_dir=\"/tmp/iris_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the training inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_inputs():\n",
    "    x = tf.constant(training_set.data)\n",
    "    y = tf.constant(training_set.target)\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then next step is to fit the model with the training data. Fitting is where the model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model/model.ckpt-4000\n",
      "INFO:tensorflow:Saving checkpoints for 4001 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.048325762, step = 4001\n",
      "INFO:tensorflow:global_step/sec: 883.463\n",
      "INFO:tensorflow:loss = 0.04824245, step = 4101 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 998.273\n",
      "INFO:tensorflow:loss = 0.048162866, step = 4201 (0.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 1022.4\n",
      "INFO:tensorflow:loss = 0.048086744, step = 4301 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 937.04\n",
      "INFO:tensorflow:loss = 0.04801396, step = 4401 (0.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 1017.14\n",
      "INFO:tensorflow:loss = 0.04794537, step = 4501 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 1058.93\n",
      "INFO:tensorflow:loss = 0.04800624, step = 4601 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 1021.99\n",
      "INFO:tensorflow:loss = 0.0497568, step = 4701 (0.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 849.098\n",
      "INFO:tensorflow:loss = 0.04782542, step = 4801 (0.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 921.329\n",
      "INFO:tensorflow:loss = 0.04771675, step = 4901 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 1006.83\n",
      "INFO:tensorflow:loss = 0.04766294, step = 5001 (0.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 1014.06\n",
      "INFO:tensorflow:loss = 0.04767774, step = 5101 (0.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 1030.3\n",
      "INFO:tensorflow:loss = 0.04896474, step = 5201 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 979.28\n",
      "INFO:tensorflow:loss = 0.047865026, step = 5301 (0.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 965.502\n",
      "INFO:tensorflow:loss = 0.047519047, step = 5401 (0.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 973.644\n",
      "INFO:tensorflow:loss = 0.04746407, step = 5501 (0.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 990.01\n",
      "INFO:tensorflow:loss = 0.047551334, step = 5601 (0.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 881.445\n",
      "INFO:tensorflow:loss = 0.048334632, step = 5701 (0.113 sec)\n",
      "INFO:tensorflow:global_step/sec: 1007.51\n",
      "INFO:tensorflow:loss = 0.047677044, step = 5801 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 1028.98\n",
      "INFO:tensorflow:loss = 0.047369454, step = 5901 (0.108 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6000 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.047326565.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(params={'head': <tensorflow.contrib.learn.python.learn.estimators.head._MultiClassHead object at 0x1c2062fd30>, 'hidden_units': [10, 20, 10], 'feature_columns': (_RealValuedColumn(column_name='', dimension=4, default_value=None, dtype=tf.float32, normalizer=None),), 'optimizer': None, 'activation_fn': <function relu at 0x1160d4d90>, 'dropout': None, 'gradient_clip_norm': None, 'embedding_lr_multipliers': None, 'input_layer_min_slice_size': None})"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the classifier with the training data\n",
    "classifier.fit(input_fn=training_inputs, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define test inputs\n",
    "def test_inputs():\n",
    "    x = tf.constant(test_set.data)\n",
    "    y = tf.constant(test_set.target)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we evaluate the accuracy of our trained model. \n",
    "We do this using the evaluate method. It takes in the test input data and target to build its input data pipeline. After measuring the model's accuracy, it returns a dictionary containing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-03-17:07:45\n",
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model/model.ckpt-6000\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-03-17:07:45\n",
      "INFO:tensorflow:Saving dict for global step 6000: accuracy = 1.0, global_step = 6000, loss = 0.022526257\n",
      "Accuracy score 1.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate the classifier's accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_inputs, steps=1)['accuracy']\n",
    "print (\"Accuracy score\", accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to see if our model can predict the type of Iris flower given a new flower sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classify two new flower samples.\n",
    "def new_flower_samples():\n",
    "    return np.array(\n",
    "        [[6.4, 3.2, 4.5, 1.5],\n",
    "        [5.8, 3.1, 5.0, 1.7]], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model/model.ckpt-6000\n",
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "# Predict the type of Iris flower\n",
    "prediction = classifier.predict_classes(input_fn=new_flower_samples)\n",
    "print (list(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We made it!!! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
