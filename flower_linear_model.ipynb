{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A linear model to discern flowers using Tensorflow\n",
    "We'll build a linear model to recognize different kinds of flowers using a canned estimator.\n",
    "\n",
    "We will use Google's Tensorflow: an open source machine learning tool for everyone.\n",
    "\n",
    "The types of flowers we would like to distunguish is Iris Versicolour, Iris Virginica, Iris Setosa. \n",
    "\n",
    "Here are the Iris flowers we'd want our model to train on\n",
    "\n",
    "* Iris Virginica\n",
    "![Iris Virginica](images/178px-Iris_virginica.jpg)\n",
    "\n",
    "* Iris Versicolor\n",
    "![Iris_versicolor](images/193px-Iris_versicolor_3.jpg)\n",
    "\n",
    "* Iris Setosa\n",
    "![Iris Kosaciec](images/109px-Kosaciec_szczecinkowaty_Iris_setosa.jpg)\n",
    "\n",
    "The data columns we have are:\n",
    "* sepal length in cm\n",
    "* sepal width in cm\n",
    "* petal length in cm\n",
    "* petal width in cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0    1    2    3  4\n",
      "71   5.5  2.3  4.0  1.3  1\n",
      "140  6.7  3.0  5.2  2.3  2\n",
      "23   6.0  2.2  4.0  1.0  1\n",
      "76   6.8  3.2  5.9  2.3  2\n",
      "131  4.8  3.0  1.4  0.3  0\n",
      "58   4.8  3.4  1.9  0.2  0\n",
      "46   6.5  3.0  5.8  2.2  2\n",
      "28   6.0  2.9  4.5  1.5  1\n",
      "80   5.8  2.7  5.1  1.9  2\n",
      "149  5.6  2.9  3.6  1.3  1\n",
      "145  6.6  2.9  4.6  1.3  1\n",
      "20   5.6  2.8  4.9  2.0  2\n",
      "1    6.5  3.0  5.2  2.0  2\n",
      "36   6.9  3.1  5.4  2.1  2\n",
      "136  6.6  3.0  4.4  1.4  1\n",
      "111  6.4  2.8  5.6  2.2  2\n",
      "56   4.9  3.1  1.5  0.1  0\n",
      "120  6.7  3.1  4.4  1.4  1\n",
      "144  5.2  4.1  1.5  0.1  0\n",
      "98   7.7  2.8  6.7  2.0  2\n",
      "81   5.0  3.5  1.6  0.6  0\n",
      "133  6.2  2.2  4.5  1.5  1\n",
      "117  7.2  3.2  6.0  1.8  2\n",
      "85   5.7  3.0  4.2  1.2  1\n",
      "65   5.1  2.5  3.0  1.1  1\n",
      "26   6.2  2.9  4.3  1.3  1\n",
      "87   5.0  3.4  1.6  0.4  0\n",
      "132  5.8  2.7  4.1  1.0  1\n",
      "130  5.9  3.0  4.2  1.5  1\n",
      "8    6.3  3.4  5.6  2.4  2\n",
      "[120, 4, 'setosa', 'versicolor', 'virginica']\n",
      "[30, 4, 'setosa', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: split the data into training and testing data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "actual_data = pd.read_csv(\"iris.csv\")\n",
    "data = pd.read_csv(\"iris.csv\", skiprows=[0], header=None)\n",
    "labels_data = pd.read_csv(\"iris.csv\", usecols=[4], skiprows=[0], header=None)\n",
    "labels = np.unique(np.array(labels_data, 'str'))\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if row[4] == labels[0]:\n",
    "        data.loc[index, 4] = 0\n",
    "    elif row[4] == labels[1]:\n",
    "        data.loc[index, 4] = 1\n",
    "    else:\n",
    "        data.loc[index, 4] = 2\n",
    "\n",
    "# shuffle the newly formatted data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "# split that data, 80-20\n",
    "y = actual_data.species\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2)\n",
    "print (X_test)\n",
    "# write the split data to csv\n",
    "# but first create a custom header\n",
    "# omit the last column since that's the one we wish to predict\n",
    "X_train_header = list((len(X_train.index), len(X_train.columns) - 1))  + list(labels)\n",
    "X_test_header = list((len(X_test.index), len(X_test.columns) - 1)) + list(labels)\n",
    "print (X_train_header)\n",
    "print (X_test_header)\n",
    "X_train.to_csv(\"iris_training.csv\", index=False, index_label=False, header=X_train_header)\n",
    "X_test.to_csv(\"iris_test.csv\", index=False, index_label=False, header=X_test_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 2 0 0 2 1 2 1 1 2 2 2 1 2 0 1 0 2 0 1 2 1 1 1 0 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "\n",
    "# Data files containing our sepal and petal features\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "\n",
    "training_set = base.load_csv_with_header(\n",
    "    filename=IRIS_TRAINING, \n",
    "    features_dtype=np.float32, \n",
    "    target_dtype=np.int)\n",
    "\n",
    "test_set = base.load_csv_with_header(\n",
    "    filename=IRIS_TEST,\n",
    "    features_dtype=np.float32,\n",
    "    target_dtype=np.int)\n",
    "\n",
    "print(test_set.target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify that all feature columns have real-value data\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build the 3 layer Deep Nueral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c1f7e7f98>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/iris_model'}\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.contrib.learn.DNNClassifier(\n",
    "    feature_columns=featured_columns,\n",
    "    hidden_units=[10, 20, 10],\n",
    "    n_classes=3,\n",
    "    model_dir=\"/tmp/iris_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the training inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_inputs():\n",
    "    x = tf.constant(training_set.data)\n",
    "    y = tf.constant(training_set.target)\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then next step is to fit the model with the training data. Fitting is where the model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Andela_9/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:192: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.5912925, step = 1\n",
      "INFO:tensorflow:global_step/sec: 827.676\n",
      "INFO:tensorflow:loss = 0.28244945, step = 101 (0.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 990.599\n",
      "INFO:tensorflow:loss = 0.12283801, step = 201 (0.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 921.432\n",
      "INFO:tensorflow:loss = 0.084290884, step = 301 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 832.59\n",
      "INFO:tensorflow:loss = 0.07436801, step = 401 (0.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 857.154\n",
      "INFO:tensorflow:loss = 0.06916182, step = 501 (0.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 964.682\n",
      "INFO:tensorflow:loss = 0.065359734, step = 601 (0.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 1063.21\n",
      "INFO:tensorflow:loss = 0.06362448, step = 701 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 840.237\n",
      "INFO:tensorflow:loss = 0.061846804, step = 801 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 938.842\n",
      "INFO:tensorflow:loss = 0.060784522, step = 901 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 837.753\n",
      "INFO:tensorflow:loss = 0.059829988, step = 1001 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 1021.78\n",
      "INFO:tensorflow:loss = 0.059000112, step = 1101 (0.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 700.564\n",
      "INFO:tensorflow:loss = 0.058343317, step = 1201 (0.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 613.758\n",
      "INFO:tensorflow:loss = 0.057743885, step = 1301 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 730.892\n",
      "INFO:tensorflow:loss = 0.057220597, step = 1401 (0.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 623.131\n",
      "INFO:tensorflow:loss = 0.05676443, step = 1501 (0.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 631.875\n",
      "INFO:tensorflow:loss = 0.05635155, step = 1601 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 614.425\n",
      "INFO:tensorflow:loss = 0.055971853, step = 1701 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 740.379\n",
      "INFO:tensorflow:loss = 0.055615094, step = 1801 (0.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 802.381\n",
      "INFO:tensorflow:loss = 0.055315897, step = 1901 (0.130 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.055006146.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(params={'head': <tensorflow.contrib.learn.python.learn.estimators.head._MultiClassHead object at 0x1c1f8f7e10>, 'hidden_units': [10, 20, 10], 'feature_columns': (_RealValuedColumn(column_name='', dimension=4, default_value=None, dtype=tf.float32, normalizer=None),), 'optimizer': None, 'activation_fn': <function relu at 0x1160d4d90>, 'dropout': None, 'gradient_clip_norm': None, 'embedding_lr_multipliers': None, 'input_layer_min_slice_size': None})"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the classifier with the training data\n",
    "classifier.fit(input_fn=training_inputs, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define test inputs\n",
    "def test_inputs():\n",
    "    x = tf.constant(test_set.data)\n",
    "    y = tf.constant(test_set.target)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we evaluate the accuracy of our trained model. \n",
    "We do this using the evaluate method. It takes in the test input data and target to build its input data pipeline. After measuring the model's accuracy, it returns a dictionary containing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-03-16:42:26\n",
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model/model.ckpt-2000\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-03-16:42:26\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 1.0, global_step = 2000, loss = 0.02180128\n",
      "Accuracy score 1.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate the classifier's accuracy\n",
    "accuracy_score = classifier.evaluate(input_fn=test_inputs, steps=1)['accuracy']\n",
    "print (\"Accuracy score\", accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to see if our model can predict the type of Iris flower given a new flower sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classify two new flower samples.\n",
    "def new_flower_samples():\n",
    "    return np.array(\n",
    "        [[6.4, 3.2, 4.5, 1.5],\n",
    "        [5.8, 3.1, 5.0, 1.7]], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the type of Iris flower\n",
    "prediction = classifier.predict(input_fn=new_flower_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
