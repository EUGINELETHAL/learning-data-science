{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "In order to understand decision trees, we have to learn about entropy. \n",
    "Entropy is the degree of randomness or disorder in the system.\n",
    "\n",
    "Let's use water ice and vapour to explain this concept. \n",
    "\n",
    "Ice has a low entropy because particles cannot move around at all, water has some particles moving around so it's medium entropy. Vapour has a higher entropy because its particles have the freedom to move around faster and further.\n",
    "\n",
    "So the more rigid(homogeneous) the set, the less entropy it will have and vice versa. Also the more knowledge one has on a set, the less entropy and vice versa. A set that has different entities has much more entropy than a set containing an entity of the same nature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters for Decision Trees\n",
    "In order to create decision trees that will generalize well, we need to tune some aspects of the trees. These aspects are called hyperparameters.\n",
    "\n",
    "Here are the most important ones:\n",
    "1. Maximum Depth –– This is the largest length between the root and a leaf. A tree with length $k$ can have at most $2^k$ leaves\n",
    "2. Minumum number of samples per leaf –– To prevent instances like having 99% of the samples being in a single leaf and only 1% on the other, we can set a minimum for the number of sample for each leaf.\n",
    "3. Maximum number of features –– most times we have too many features to build a tree. On every split, it would be expensive to check the entire dataset on each of the features. A solution for this is to limit the number of features that we look for in each split.\n",
    "\n",
    "Points to note:\n",
    "* Large depths can cause overfitting –– a tree that is too deep can memorize data. we don't want that.\n",
    "* Small depths can cause underfitting –– a tree that is too small can result in a very simple model.\n",
    "* Small minimum samples per leaf –– results in leafs with very small samples which results in the tree memorizing data/overfitting\n",
    "* Large minimum samples per lead –– results in leafs that have very large samples \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
