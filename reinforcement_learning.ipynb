{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "* The Reinforcement learning is characterized by an **agent** learning to interact with its **environment**.\n",
    "* At each time step, The environment presents a situation to the agent called a **state**. The agent then has to choose an appropriate **action** in response.  \n",
    "* One time step later, the agent receives a **reward**. A reward can be negative or positive. This is the environment's way of telling the agent whether it has responded with an appropriate action or not. The agent also receives a new state.\n",
    "* Now, the main goal of the agent is to maximize expected **cumulative reward**. This simply means the expected sum of rewards attained over all the time steps.\n",
    "* The reward hypothesis is that all goals for any agent are framed as the **maximization of the expected cumulative reward**.\n",
    "\n",
    "At an arbitrary time step $t$, the agent-environment interaction is simply a bunch of states, actions and rewards\n",
    "\n",
    "$(S_0,A_0,R_1,S_1,A_1,,R_{t−1},S_{t−1},A_{t−1},R_t,S_t,A_t)$\n",
    "\n",
    "![](images/reinforcement-agent.png \"The Agent-Environment Interaction\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we implement Reinforcement Learning?\n",
    "Enter Markov Decision Processes (MDP). We define our RL problem in the form of an MDP. MDPs provide a framework to model a decision making process for the agent.\n",
    "\n",
    "But what are they, you ask? Simple: An MDP can be defined follows:\n",
    "* A (finite) set of states S\n",
    "* A (finite) set of actions A (The actions available to the agent)\n",
    "* A (finite) set of rewards R (The reward the agent will get after transitioning from one state to another)\n",
    "* The one-step dynamics of the environment and,\n",
    "* A discount rate $\\gamma$ (gamma) where $0≤\\gamma≤1$.\n",
    "\n",
    "Points to note:\n",
    "\n",
    "The discount rate $\\gamma$ represents the degree of importance between present and future rewards the agent gets. \n",
    "\n",
    "But why are we doing a discounted return in the first place? \n",
    "\n",
    "The main aim of doing this is to **refine the goal** you have for the agent. \n",
    "\n",
    "If $\\gamma = 0$, the agent will only care about the immediate reward.\n",
    "\n",
    "If $\\gamma = 1$, then the return is not discounted. \n",
    "\n",
    "This means that the  $\\gamma$ has to be close to 1. \n",
    "\n",
    "Should the present reward carry the same weight as future rewards? No. It actually makes more sense to value rewards that come sooner more highly since they are more predictable. The closer in time the reward is to the agent the more juicier it is to it!\n",
    "\n",
    "Therefore, the larger the discount rate is, the larger the immediate reward.\n",
    "\n",
    "$ G_t = \\mathopen{} R_{t + 1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\text{... } $\n",
    "\n",
    "If we replace the values for example, we can have it as:\n",
    "\n",
    "$ G_t = \\mathopen{} R_{t + 1} + (0.9) R_{t + 2} + (0.82) R_{t + 3} + \\text{... } $\n",
    "\n",
    "Moving on swiftly.\n",
    "\n",
    "Now, you must be wondering what _the one-step dynamics of the environment_ is all about. \n",
    "\n",
    "Well, it's purpose is to help the environment decide the state and rewards the agent gets at every time step.\n",
    "\n",
    "When the environment responds to the agent at time step $t+1$, **it considers only the state and action at the previous time step\n",
    "$(S_t, A_t)$**. \n",
    "\n",
    "This means that it doesn't care or look at the actions, rewards or states **that came prior to the last time step**. This dictates the one-step dynamics of the environment. \n",
    "\n",
    "It is therefore a conditional probability ($P(A \\mid B)$ –– meaning we find the probability that event A will happen given event B has already occured). The one-step dynamic of the environment is defined as follows:\n",
    "\n",
    "$$ P(s', r \\mid s, a) = P\\bigl(S_{t+1} = s', R_{t + 1} = r \\mid S_t = s, A_t = a) \\text{ for each possible } s, r, s' \\text { and } a $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving MDPs\n",
    "Now that we've learnt to define a problem into an MDP, how does the Agent decide which actions to take given its states?\n",
    "This is where the **policy** comes in.  \n",
    "The core problem is to come up with a policy that'll help the agent map the set of states to the set of actions it will take. \n",
    "A policy is simply a function $\\pi$ that specifies the action **$\\pi(s)$** the agent will choose when state **$s$**.\n",
    "\n",
    "Therefore, in order to solve an MDP, the agent must determine the best policy. The best policy will be an optimal policy that tells the agent to select actions so that it always gets the highest possible cumulative reward.\n",
    "\n",
    "There are two ways of defining a policy:\n",
    "* Deterministic policy –– this is a mapping $ \\pi:S \\xrightarrow\\ A $\n",
    "* Stochastic policy –– this uses probability. For each state $s$ and action $a$, it creates a probability $\\pi(a \\mid s)$ that the agent chooses action $a$ while in state $s$\n",
    "    $$ \\pi(a \\mid s) = \\Pr\\bigl(A_t = a \\mid S_t = s) $$\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
