{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "* The Reinforcement learning is characterized by an **agent** learning to interact with its **environment**.\n",
    "* At each time step, The environment presents a situation to the agent called a **state**. The agent then has to choose an appropriate **action** in response.  \n",
    "* One time step later, the agent receives a **reward**. A reward can be negative or positive. This is the environment's way of telling the agent whether it has responded with an appropriate action or not. The agent also receives a new state.\n",
    "* Now, the main goal of the agent is to maximize expected **cumulative reward**. This simply means the expected sum of rewards attained over all the time steps.\n",
    "* The reward hypothesis is that all goals for any agent are framed as the **maximization of the expected cumulative reward**.\n",
    "\n",
    "#### So what mathematical framework does the agents use to learn?\n",
    "Enter Markov Decision Processes (MDP).\n",
    "\n",
    "What are they, you ask? Simple.\n",
    "An MDP can be defined as:\n",
    "* A (finite) set of states S\n",
    "* A (finite) set of actions A\n",
    "* A (finite) set of rewards R\n",
    "* The one-step dynamics of the environment and\n",
    "* A discount rate γ(gamma) where 0≤γ≤1. The gamma has to be close to 1.\n",
    "If γ = 0, the agent will only care about the immediate reward.\n",
    "If γ = 1, then the return is not discounted. \n",
    "But why are we doing a discounted return? The main aim of this is to refine the goal you have for the agent. \n",
    "\n",
    "Now, you might wonder what the hell a one-step dynamic of the environment is for. It's purpose is to help the environment decide the state and rewards the agent gets at every time step.\n",
    "Formally, it is a conditional probability (P(A|B) == The probability that event A will happen given event B has already occured). It's then defined as follows:\n",
    "\n",
    "$$\n",
    "p(s \n",
    "′\n",
    " ,r∣s,a)≐P(S \n",
    "t+1\n",
    "​\t =s \n",
    "′\n",
    " ,R \n",
    "t+1\n",
    "​\t =r∣S \n",
    "t\n",
    "​\t =s,A \n",
    "t\n",
    "​\t =a) \n",
    "​\n",
    "​   for each possible \n",
    "​ s', r, s, \\text{and } a\n",
    ". $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving MDPs\n",
    "Now that we've learnt to define a problem into an MDP, how does the Agent decide which actions to take given its states?\n",
    "This is where the **policy** comes in.  \n",
    "The core problem is to come up with a policy that'll help the agent map the set of states to the set of actions it will take. \n",
    "A policy is simply a function $π$ that specifies the action **$π(s)$** the agent will choose when state **$s$**.\n",
    "\n",
    "Therefore, in order to solve an MDP, the agent must determine the best policy. The best policy will be an optimal policy that tells the agent to select actions so that it always gets the highest possible cumulative reward.\n",
    "\n",
    "There are two ways of defining a policy:\n",
    "* Deterministic policy –– this is a mapping $ π:S \\xrightarrow\\ A $\n",
    "* Stochastic policy –– this uses probability. For each state $s$ and action $a$, it creates a probability $π(a \\mid s)$ that the agent chooses action $a$ while in state $s$\n",
    "    $$ π(a \\mid s) = \\Pr\\bigl(A_t = a \\mid S_t = s) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
