{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning (Learning like humans do)\n",
    "Like humans, agents learn for themselves. \n",
    "\n",
    "They find successful strategies that lead to the greatest long-term rewards. This concept of learning by trial-and-error, like humans do, and solely from rewards or punishments, is known as reinforcement learning (RL).\n",
    "\n",
    "* The Reinforcement learning is characterized by an **agent** learning to interact with its **environment**.\n",
    "* At each time step, the environment presents a situation to the agent called a **state**. The agent then has to choose an appropriate **action** in response.  \n",
    "* One time step later, the agent receives a **reward**. A reward can be negative or positive. This is the environment's way of telling the agent whether it has responded with an appropriate action or not. The agent also receives a new state.\n",
    "\n",
    "                    This is how the agent-environment interaction looks like:\n",
    "![](images/reinforcement-agent.png \"The Agent-Environment Interaction\")\n",
    "\n",
    "* Now, the main goal of the agent is to maximize expected **cumulative reward**. This simply means the expected sum of rewards attained over all the time steps.\n",
    "* The reward hypothesis is that all goals for any agent are framed as the **maximization of the expected cumulative reward**.\n",
    "\n",
    "At an arbitrary time step $t$, the agent-environment interaction is simply a bunch of states, actions and rewards like so:\n",
    "\n",
    "$\\bigl(S_0, A_0, R_1, S_1, A_1, R_2, ..., R_{t - 1}, S_{ t - 1}, A_ {t - 1}, R_t, S_t, A_t  \\bigr)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we implement Reinforcement Learning?\n",
    "Enter Markov Decision Processes (MDP). We define our RL problem in the form of an MDP. MDPs provide a framework to model a decision making process for the agent.\n",
    "\n",
    "But what are they, you ask? Simple: An MDP can be defined follows:\n",
    "* A (finite) set of states S\n",
    "* A (finite) set of actions A (The actions available to the agent)\n",
    "* A (finite) set of rewards R (The reward the agent will get after transitioning from one state to another)\n",
    "* The one-step dynamics of the environment and,\n",
    "* A discount rate $\\gamma$ (gamma) where $0\\leq\\gamma\\leq1$.\n",
    "\n",
    "Points to note:\n",
    "\n",
    "The discount rate $\\gamma$ represents the degree of importance between present and future rewards the agent gets. \n",
    "\n",
    "But why are we doing a discounted return in the first place? \n",
    "\n",
    "The main aim of doing this is to **refine the goal** you have for the agent. \n",
    "\n",
    "If $\\gamma = 0$, the agent will only care about the immediate reward.\n",
    "\n",
    "If $\\gamma = 1$, then the return is not discounted. \n",
    "\n",
    "This means that the  $\\gamma$ has to be close to 1. \n",
    "\n",
    "Should the present reward carry the same weight as future rewards? No. It actually makes more sense to value rewards that come sooner more highly since they are more predictable. The closer in time the reward is to the agent the more juicier it is to it!\n",
    "\n",
    "Therefore, the larger the discount rate is, the larger the immediate reward.\n",
    "\n",
    "$ G_t = \\mathopen{} R_{t + 1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\text{... } $\n",
    "\n",
    "If we replace the values for example, we can have it as:\n",
    "\n",
    "$ G_t = \\mathopen{} R_{t + 1} + (0.9) R_{t + 2} + (0.82) R_{t + 3} + \\text{... } $\n",
    "\n",
    "Moving on swiftly.\n",
    "\n",
    "Now, you must be wondering what _the one-step dynamics of the environment_ is all about. \n",
    "\n",
    "Well, it's purpose is to help the environment decide the state and rewards the agent gets at every time step.\n",
    "\n",
    "When the environment responds to the agent at time step $t+1$, **it considers only the state and action at the previous time step\n",
    "$(S_t, A_t)$**. \n",
    "\n",
    "This means that it doesn't care or look at the actions, rewards or states **that came prior to the last time step**. This dictates the one-step dynamics of the environment. \n",
    "\n",
    "It is therefore a conditional probability ($P(A \\mid B)$ –– meaning we find the probability that event A will happen given event B has already occured). The one-step dynamic of the environment is defined as follows:\n",
    "\n",
    "$$ P(s', r \\mid s, a) = P\\bigl(S_{t+1} = s', R_{t + 1} = r \\mid S_t = s, A_t = a) \\text{ for each possible } s, r, s' \\text { and } a $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving MDPs\n",
    "Now that we've learnt to define a problem into an MDP, how does the Agent decide which actions to take given its states?\n",
    "This is where the **policy** comes in.  \n",
    "The core problem is to come up with a policy that'll help the agent map the set of states to the set of actions it will take. \n",
    "A policy is simply a function $\\pi$ that specifies the action **$\\pi(s)$** the agent will choose when state **$s$**.\n",
    "\n",
    "Therefore, in order to solve an MDP, the agent must determine the best policy. The best policy will be an optimal policy that tells the agent to select actions so that it always gets the highest possible cumulative reward.\n",
    "\n",
    "There are two ways of defining a policy:\n",
    "* Deterministic policy –– this is a mapping $ \\pi:S \\xrightarrow\\ A $\n",
    "* Stochastic policy –– this uses probability. For each state $s$ and action $a$, it creates a probability $\\pi(a \\mid s)$ that the agent chooses action $a$ while in state $s$\n",
    "    $$ \\pi(a \\mid s) = \\Pr\\bigl(A_t = a \\mid S_t = s) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we do this in code?\n",
    "We can therefore use the stochastic policy on a neural network, where the network will return the probability that the agent should select each possible action.\n",
    "\n",
    "We will incorporate the policy of the MPD as a neural network. The environment's state will be passed in as **input** and the output will be the probabilities of actions to be selected. \n",
    "\n",
    "For the agent to select an action, it chooses an action that has the highest probability. \n",
    "\n",
    "The reward is then given and the environment presents a new state to the agent.(For a game, the reward == score, the state = new level or updated screen with a different challenge or something)\n",
    "\n",
    "First, the network will initialize the weights for the policy at random. It will learn to select the best weights eventually making the policy optimal.\n",
    "\n",
    "#### Tools we need\n",
    "* [OpenAI Gym](https://gym.openai.com/docs/) –– This library is a collection of test problems(environments) that you can use to develop and compare reinforcement learning algorithms. The environments come with a shared interface, which allows us to write general algorithms. Install it by running `pip install gym`\n",
    "* [PyTorch](https://pytorch.org/) –– A deep learning framework for fast, flexible experimentation on Tensors and Dynamic neural networks in Python. Install it by running `pip install torch`\n",
    "* [Matplotlib](https://matplotlib.org/) –– A Python 2D plotting library to be used for plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the coding begin!\n",
    "#### Step 1: Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import deque\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Step 2: Initialize a random agent\n",
    "It's important to initialize a random agent so that we can see how the agent behaves and observe the cumulative reward(measured as the score) we get when we run an episodal task. Let's go ahead and create a random agent and see how it behaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 13\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD8CAYAAACW/ATfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABGVJREFUeJzt3MFtE0EYgNFd5CaoA8qgDqempA7KgDooY7n4kDgBVkrweOd7T8rBPqz+Q/xlNDPOum3bAsDcPo0eAID/T+wBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIOA0eoALX+MFeG39qAdZ2QMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QMBp9ABwr34+Pbx4/eX8OGgSeD8re9jpOv5wJGIPECD28AareGYj9gABYg8QIPawk9s4HJnYAwSIPUCA2MMVN3GYkdjDDvbrOTqxBwgQe4AAsYdn7NczK7EHCBB7gACxh39wE4cZiD1AgNjDhcNZZib2AAFiDxAg9vAXDmeZhdgDBIg9QIDYw+ImDvMTe4AAsYc/cDjLTMQeIEDsAQLEnjyHsxSIPUCA2MMbHM4yG7EHCBB70uzXUyH2AAFiDxAg9gABYg9X3MRhRmIPECD2ZLmJQ4nYAwSIPTxjv55ZiT1AgNgDBIg9SQ5nqRF7gACxhwuHs8xM7AECxJ4c+/UUiT1AgNgDBIg9QIDYw+ImDvMTe4AAsSfFTRyqxB4gQOwBAsSePIezFIg9QIDYk+FwljKx59DWdd39895nwJGJPWlfH55GjwA3cRo9ANzK91/nF6+/fRZ6OqzsSfjxeH713nX8YWZiT4KwUyf2JNiyoU7syfIHgJJ127bRMyzLstzFEBzPLa9E3slnhZYP+wW3sgcIEHuAALEHCBB7gACxBwgQe4AAsQcIEHuAALEHCPAvjjk032qFfazsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECTqMHuFhHDwAwMyt7gACxBwgQe4AAsQcIEHuAALEHCBB7gACxBwgQe4AAsQcIEHuAALEHCBB7gACxBwgQe4AAsQcIEHuAALEHCBB7gACxBwgQe4AAsQcIEHuAgN/Hc0PU0xd1rgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for t_episode in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    img.set_data(env.render(mode='rgb_array'))\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        print('Score:', t_episode + 1)\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define the Architecture of the Policy\n",
    "We will define a neural network that encodes a stochastic policy.\n",
    "\n",
    "The network that we'll define takes the  envrironment's **state as input** and returns the probability that the agent should select each possible action as the output. The output will be two numbers say [0.8, 0.2]. This means that the agent will pushe the pole to the ledt with 80% probability. Afte selecting this action, it sends the action to the environment and receives the next state and a reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we initialize the NN, all weights are random. The goal of the agent is to figure out the appropriate weights in the NN, so that for each state, the NN always outputs probabilities that help the agent get a high score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "action space: Discrete(2)\n",
      "observation space: Box(4,)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(\"action space:\", env.action_space)\n",
    "print(\"observation space:\", env.observation_space)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size=4, h_size=16, a_size=2):\n",
    "        \"\"\"\n",
    "        Neural network that encodes the policy\n",
    "        \n",
    "        Params\n",
    "        s_size(int): dimension of each state (size of input layer)\n",
    "        h_size(int): size of hidden layer\n",
    "        a_size(int): number of potential actions (size of output layer)\n",
    "        \"\"\"\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # activation functions is ReLu (Rectified Linear unit)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Train the agent\n",
    "We'll use the REINFORCE algorithm(Monte Carlo Policy Gradients) to help the agent train the weights of the neural network while it plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\t Average Score:  22.18\n",
      "Episode 200\t Average Score:  29.66\n",
      "Episode 300\t Average Score:  68.12\n",
      "Episode 400\t Average Score:  122.18\n",
      "Episode 500\t Average Score:  177.55\n",
      "Episode 600\t Average Score:  167.32\n",
      "Episode 700\t Average Score:  189.76\n",
      "Episode 800\t Average Score:  191.59\n",
      "Environment solved in 718 episodes. \tAverage Score: 195.23\n",
      "Environment solved in 719 episodes. \tAverage Score: 195.23\n",
      "Environment solved in 720 episodes. \tAverage Score: 195.22\n",
      "Environment solved in 721 episodes. \tAverage Score: 195.32\n",
      "Environment solved in 722 episodes. \tAverage Score: 195.99\n",
      "Environment solved in 723 episodes. \tAverage Score: 195.99\n",
      "Environment solved in 724 episodes. \tAverage Score: 195.99\n",
      "Environment solved in 725 episodes. \tAverage Score: 195.99\n",
      "Environment solved in 726 episodes. \tAverage Score: 196.44\n",
      "Environment solved in 727 episodes. \tAverage Score: 196.44\n",
      "Environment solved in 728 episodes. \tAverage Score: 196.44\n",
      "Environment solved in 729 episodes. \tAverage Score: 196.44\n",
      "Environment solved in 730 episodes. \tAverage Score: 196.44\n",
      "Environment solved in 731 episodes. \tAverage Score: 196.44\n",
      "Environment solved in 732 episodes. \tAverage Score: 196.44\n",
      "Environment solved in 733 episodes. \tAverage Score: 196.89\n",
      "Environment solved in 734 episodes. \tAverage Score: 197.45\n",
      "Environment solved in 735 episodes. \tAverage Score: 197.57\n",
      "Environment solved in 736 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 737 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 738 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 739 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 740 episodes. \tAverage Score: 198.05\n",
      "Environment solved in 741 episodes. \tAverage Score: 198.08\n",
      "Environment solved in 742 episodes. \tAverage Score: 198.08\n",
      "Environment solved in 743 episodes. \tAverage Score: 198.08\n",
      "Environment solved in 744 episodes. \tAverage Score: 198.08\n",
      "Environment solved in 745 episodes. \tAverage Score: 198.08\n",
      "Environment solved in 746 episodes. \tAverage Score: 198.18\n",
      "Environment solved in 747 episodes. \tAverage Score: 198.18\n",
      "Environment solved in 748 episodes. \tAverage Score: 198.18\n",
      "Environment solved in 749 episodes. \tAverage Score: 198.18\n",
      "Environment solved in 750 episodes. \tAverage Score: 198.18\n",
      "Environment solved in 751 episodes. \tAverage Score: 198.18\n",
      "Environment solved in 752 episodes. \tAverage Score: 198.18\n",
      "Environment solved in 753 episodes. \tAverage Score: 198.18\n",
      "Environment solved in 754 episodes. \tAverage Score: 198.18\n",
      "Environment solved in 755 episodes. \tAverage Score: 197.82\n",
      "Environment solved in 756 episodes. \tAverage Score: 197.82\n",
      "Environment solved in 757 episodes. \tAverage Score: 197.82\n",
      "Environment solved in 758 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 759 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 760 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 761 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 762 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 763 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 764 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 765 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 766 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 767 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 768 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 769 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 770 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 771 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 772 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 773 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 774 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 775 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 776 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 777 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 778 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 779 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 780 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 781 episodes. \tAverage Score: 197.89\n",
      "Environment solved in 782 episodes. \tAverage Score: 198.02\n",
      "Environment solved in 783 episodes. \tAverage Score: 198.02\n",
      "Environment solved in 784 episodes. \tAverage Score: 198.02\n",
      "Environment solved in 785 episodes. \tAverage Score: 198.02\n",
      "Environment solved in 786 episodes. \tAverage Score: 198.02\n",
      "Environment solved in 787 episodes. \tAverage Score: 198.02\n",
      "Environment solved in 788 episodes. \tAverage Score: 198.02\n",
      "Environment solved in 789 episodes. \tAverage Score: 198.02\n",
      "Environment solved in 790 episodes. \tAverage Score: 198.02\n",
      "Environment solved in 791 episodes. \tAverage Score: 198.02\n",
      "Environment solved in 792 episodes. \tAverage Score: 198.02\n",
      "Environment solved in 793 episodes. \tAverage Score: 198.02\n",
      "Environment solved in 794 episodes. \tAverage Score: 198.02\n",
      "Environment solved in 795 episodes. \tAverage Score: 198.02\n",
      "Environment solved in 796 episodes. \tAverage Score: 198.02\n",
      "Environment solved in 797 episodes. \tAverage Score: 198.02\n",
      "Environment solved in 798 episodes. \tAverage Score: 198.02\n",
      "Environment solved in 799 episodes. \tAverage Score: 198.58\n",
      "Episode 900\t Average Score:  198.69\n",
      "Environment solved in 800 episodes. \tAverage Score: 198.69\n",
      "Environment solved in 801 episodes. \tAverage Score: 198.69\n",
      "Environment solved in 802 episodes. \tAverage Score: 198.69\n",
      "Environment solved in 803 episodes. \tAverage Score: 198.69\n",
      "Environment solved in 804 episodes. \tAverage Score: 198.84\n",
      "Environment solved in 805 episodes. \tAverage Score: 198.84\n",
      "Environment solved in 806 episodes. \tAverage Score: 199.09\n",
      "Environment solved in 807 episodes. \tAverage Score: 199.09\n",
      "Environment solved in 808 episodes. \tAverage Score: 199.09\n",
      "Environment solved in 809 episodes. \tAverage Score: 199.15\n",
      "Environment solved in 810 episodes. \tAverage Score: 199.15\n",
      "Environment solved in 811 episodes. \tAverage Score: 199.15\n",
      "Environment solved in 812 episodes. \tAverage Score: 199.30\n",
      "Environment solved in 813 episodes. \tAverage Score: 199.30\n",
      "Environment solved in 814 episodes. \tAverage Score: 199.43\n",
      "Environment solved in 815 episodes. \tAverage Score: 199.43\n",
      "Environment solved in 816 episodes. \tAverage Score: 199.43\n",
      "Environment solved in 817 episodes. \tAverage Score: 199.43\n",
      "Environment solved in 818 episodes. \tAverage Score: 199.43\n",
      "Environment solved in 819 episodes. \tAverage Score: 199.43\n",
      "Environment solved in 820 episodes. \tAverage Score: 199.44\n",
      "Environment solved in 821 episodes. \tAverage Score: 199.44\n",
      "Environment solved in 822 episodes. \tAverage Score: 199.44\n",
      "Environment solved in 823 episodes. \tAverage Score: 199.44\n",
      "Environment solved in 824 episodes. \tAverage Score: 199.44\n",
      "Environment solved in 825 episodes. \tAverage Score: 199.44\n",
      "Environment solved in 826 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 827 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 828 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 829 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 830 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 831 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 832 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 833 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 834 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 835 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 836 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 837 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 838 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 839 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 840 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 841 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 842 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 843 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 844 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 845 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 846 episodes. \tAverage Score: 199.64\n",
      "Environment solved in 847 episodes. \tAverage Score: 198.54\n",
      "Environment solved in 848 episodes. \tAverage Score: 198.54\n",
      "Environment solved in 849 episodes. \tAverage Score: 198.54\n",
      "Environment solved in 850 episodes. \tAverage Score: 198.54\n",
      "Environment solved in 851 episodes. \tAverage Score: 198.54\n",
      "Environment solved in 852 episodes. \tAverage Score: 198.54\n",
      "Environment solved in 853 episodes. \tAverage Score: 198.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment solved in 854 episodes. \tAverage Score: 198.54\n",
      "Environment solved in 855 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 856 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 857 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 858 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 859 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 860 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 861 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 862 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 863 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 864 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 865 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 866 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 867 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 868 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 869 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 870 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 871 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 872 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 873 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 874 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 875 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 876 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 877 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 878 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 879 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 880 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 881 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 882 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 883 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 884 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 885 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 886 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 887 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 888 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 889 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 890 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 891 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 892 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 893 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 894 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 895 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 896 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 897 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 898 episodes. \tAverage Score: 198.90\n",
      "Environment solved in 899 episodes. \tAverage Score: 198.90\n",
      "Episode 1000\t Average Score:  198.90\n",
      "Environment solved in 900 episodes. \tAverage Score: 198.90\n"
     ]
    }
   ],
   "source": [
    "env.seed(0)\n",
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "\n",
    "def reinforce(n_episodes=1000, max_t=200, gamma=1.0, print_every=100):\n",
    "    \"\"\"\n",
    "    Pytorch impelementation of the REINFORCE algorithm\n",
    "    \n",
    "    Params\n",
    "    n_episodes(int): maximum number of training episodes\n",
    "    max_t(int): maximum number of timesteps per episode\n",
    "    gamma(float): discount rate\n",
    "    print_every (int): how often to print average score (over last 100 episodes)\n",
    "    \"\"\"\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for episode in range(1, n_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        for i in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        discounts = [gamma**i  for i in range(len(rewards) + 1)]\n",
    "        R = sum([a*b for a,b in zip(discounts, rewards)])\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if episode % print_every == 0:\n",
    "            print('Episode {}\\t Average Score:  {:.2f}'.format(episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque) >= 195.0:\n",
    "            print('Environment solved in {:d} episodes. \\tAverage Score: {:.2f}'.format(episode-100, np.mean(scores_deque)))\n",
    "    \n",
    "    return scores\n",
    "\n",
    "scores = reinforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Plot the Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores) + 1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Watch the agent play\n",
    "Finally, we'll watch the trained agent perform better on it's environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  200\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD8CAYAAACW/ATfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABFtJREFUeJzt3N1x00AYQFGJcRPUAWVQR1JTUgdlQB2UIV4ymTg/IBMTaXXPefKDRvM92Nc7mtXOy7JMABzbp60HAOD/E3uAALEHCBB7gACxBwgQe4AAsQcIEHuAALEHCDhtPcADr/ECvDRf60ZW9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAaetB4Br+Hl/+/j5y83dhpPAPs3Lsmw9wzRN0y6GYExPQ/+U6HMA87Vu5DEOQIDYAwSIPUCA2AMEiD1AgNhzSHbiwDmxBwgQe4AAsQcIEHuAALFnaG8dlQCcE3uAALEHCBB7gACxBwgQe4AAsedwHJUAL4k9QIDYAwSIPUCA2AMEiD3DclQCrCf2AAFiDxAg9hyKPfbwOrEHCBB7gACxBwgQe4AAsWdI9tjDZcQeIEDsAQLEHiBA7AECxB4gQOw5DEclwNvEHiBA7AECxB4gQOwBAsSe4TgqAS4n9gABYg8QIPYAAWIPECD2AAFizyE4KgH+TOwBAsQeIEDsAQLEHiBA7BmKoxLg34g9QIDYAwSIPcOzxx7+TuwBAsQeIEDsAQLEHiDgtPUAtM3zvPraH3c3777Hsiyrr4UjsbIHCLCyZyjff52v7r99vt9oEhiLlT3DeB56YD2xZ2j+AGAdsWdoHuPAOmLPMF4L+9dbsYc15p1sRdvFEHy8S7ZNXsNOvu+w1tV+IFb2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABjjhmU95ohY9hZQ8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPEHDaeoAH89YDAByZlT1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUDAbxuQOSzRKDOkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for t in range(1000):\n",
    "    action, _ = policy.act(state)\n",
    "    img.set_data(env.render(mode='rgb_array')) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        print('Score: ', t+1)\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
