{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "* The Reinforcement learning is characterized by an **agent** learning to interact with its **environment**.\n",
    "* At each time step, the environment presents a situation to the agent called a **state**. The agent then has to choose an appropriate **action** in response.  \n",
    "* One time step later, the agent receives a **reward**. A reward can be negative or positive. This is the environment's way of telling the agent whether it has responded with an appropriate action or not. The agent also receives a new state.\n",
    "\n",
    "                    This is how the agent-environment interaction looks like:\n",
    "![](images/reinforcement-agent.png \"The Agent-Environment Interaction\")\n",
    "\n",
    "* Now, the main goal of the agent is to maximize expected **cumulative reward**. This simply means the expected sum of rewards attained over all the time steps.\n",
    "* The reward hypothesis is that all goals for any agent are framed as the **maximization of the expected cumulative reward**.\n",
    "\n",
    "At an arbitrary time step $t$, the agent-environment interaction is simply a bunch of states, actions and rewards like so:\n",
    "\n",
    "$\\bigl(S_0, A_0, R_1, S_1, A_1, R_2, ..., R_{t - 1}, S_{ t - 1}, A_ {t - 1}, R_t, S_t, A_t  \\bigr)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we implement Reinforcement Learning?\n",
    "Enter Markov Decision Processes (MDP). We define our RL problem in the form of an MDP. MDPs provide a framework to model a decision making process for the agent.\n",
    "\n",
    "But what are they, you ask? Simple: An MDP can be defined follows:\n",
    "* A (finite) set of states S\n",
    "* A (finite) set of actions A (The actions available to the agent)\n",
    "* A (finite) set of rewards R (The reward the agent will get after transitioning from one state to another)\n",
    "* The one-step dynamics of the environment and,\n",
    "* A discount rate $\\gamma$ (gamma) where $0\\leq\\gamma\\leq1$.\n",
    "\n",
    "Points to note:\n",
    "\n",
    "The discount rate $\\gamma$ represents the degree of importance between present and future rewards the agent gets. \n",
    "\n",
    "But why are we doing a discounted return in the first place? \n",
    "\n",
    "The main aim of doing this is to **refine the goal** you have for the agent. \n",
    "\n",
    "If $\\gamma = 0$, the agent will only care about the immediate reward.\n",
    "\n",
    "If $\\gamma = 1$, then the return is not discounted. \n",
    "\n",
    "This means that the  $\\gamma$ has to be close to 1. \n",
    "\n",
    "Should the present reward carry the same weight as future rewards? No. It actually makes more sense to value rewards that come sooner more highly since they are more predictable. The closer in time the reward is to the agent the more juicier it is to it!\n",
    "\n",
    "Therefore, the larger the discount rate is, the larger the immediate reward.\n",
    "\n",
    "$ G_t = \\mathopen{} R_{t + 1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\text{... } $\n",
    "\n",
    "If we replace the values for example, we can have it as:\n",
    "\n",
    "$ G_t = \\mathopen{} R_{t + 1} + (0.9) R_{t + 2} + (0.82) R_{t + 3} + \\text{... } $\n",
    "\n",
    "Moving on swiftly.\n",
    "\n",
    "Now, you must be wondering what _the one-step dynamics of the environment_ is all about. \n",
    "\n",
    "Well, it's purpose is to help the environment decide the state and rewards the agent gets at every time step.\n",
    "\n",
    "When the environment responds to the agent at time step $t+1$, **it considers only the state and action at the previous time step\n",
    "$(S_t, A_t)$**. \n",
    "\n",
    "This means that it doesn't care or look at the actions, rewards or states **that came prior to the last time step**. This dictates the one-step dynamics of the environment. \n",
    "\n",
    "It is therefore a conditional probability ($P(A \\mid B)$ –– meaning we find the probability that event A will happen given event B has already occured). The one-step dynamic of the environment is defined as follows:\n",
    "\n",
    "$$ P(s', r \\mid s, a) = P\\bigl(S_{t+1} = s', R_{t + 1} = r \\mid S_t = s, A_t = a) \\text{ for each possible } s, r, s' \\text { and } a $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving MDPs\n",
    "Now that we've learnt to define a problem into an MDP, how does the Agent decide which actions to take given its states?\n",
    "This is where the **policy** comes in.  \n",
    "The core problem is to come up with a policy that'll help the agent map the set of states to the set of actions it will take. \n",
    "A policy is simply a function $\\pi$ that specifies the action **$\\pi(s)$** the agent will choose when state **$s$**.\n",
    "\n",
    "Therefore, in order to solve an MDP, the agent must determine the best policy. The best policy will be an optimal policy that tells the agent to select actions so that it always gets the highest possible cumulative reward.\n",
    "\n",
    "There are two ways of defining a policy:\n",
    "* Deterministic policy –– this is a mapping $ \\pi:S \\xrightarrow\\ A $\n",
    "* Stochastic policy –– this uses probability. For each state $s$ and action $a$, it creates a probability $\\pi(a \\mid s)$ that the agent chooses action $a$ while in state $s$\n",
    "    $$ \\pi(a \\mid s) = \\Pr\\bigl(A_t = a \\mid S_t = s) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we do this in code?\n",
    "We can therefore use the stochastic policy on a neural network, where the network will return the probability that the agent should select each possible action.\n",
    "\n",
    "We will incorporate the policy of the MPD as a neural network. The environment's state will be passed in as **input** and the output will be the probabilities of actions to be selected. \n",
    "\n",
    "For the agent to select an action, it chooses an action that has the highest probability. \n",
    "\n",
    "The reward is then given and the environment presents a new state to the agent.(For a game, the reward == score, the state = new level or updated screen with a different challenge or something)\n",
    "\n",
    "First, the network will initialize the weights for the policy at random. It will learn to select the best weights eventually making the policy optimal.\n",
    "\n",
    "#### Tools we need\n",
    "* [OpenAI Gym](https://gym.openai.com/docs/) –– This library is a collection of test problems(environments) that you can use to develop and compare reinforcement learning algorithms. The environments come with a shared interface, which allows us to write general algorithms. Install it by running `pip install gym`\n",
    "* [PyTorch](https://pytorch.org/) –– A deep learning framework for fast, flexible experimentation on Tensors and Dynamic neural networks in Python. Install it by running `pip install torch`\n",
    "* [Matplotlib](https://matplotlib.org/) –– A Python 2D plotting library to be used for plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the coding begin!\n",
    "#### Step 1: Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import deque\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Step 2: Initialize a random agent\n",
    "It's important to initialize a random agent so that we can see how the agent behaves and observe the cumulative reward(measured as the score) we get when we run an episodal task. Let's go ahead and create a random agent and see how it behaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 12\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD8CAYAAACW/ATfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABGVJREFUeJzt3Mtt20AUQFEyUBOpI22kDqcNt2HXkTKcOlKGstEi0MehZDkU556zFGTjLayLAefJ836/nwAY25e1BwDg84k9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwTs1h7gwNd4AU7N9/pFTvYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPECD2AAFiDxCwW3sAWMuv1x9nX//29PKfJ4HP52RPlqhTIvYAAWIPECD2cOTSs3zYMrEHCBB7gACxBwgQe9KsX1Ih9gABYg9n2MhhNGIPECD25HluT4HYAwSIPUCA2AMEiD1AgNjDBdYvGYnYAwSIPUzWLxmf2AMEiD1AgNjDO1zSMgqxBwgQe4AAsYcDGzmMTOwBAsQeIEDs4R9s5DACsQcIEHuAALGHv9jIYVRiDxAg9gABYg8L2Mhh68QeIEDs4YhLWkYk9gABYg8QIPawkEtatkzsAQLEHiBA7AECxB7OsH7JaMQeIEDs4Qo2ctgqsQcIEHuAALEHCBB7uMBGDiMRe4AAsQcIEHu4kvVLtkjsAQLEHt7hkpZRiD1AgNgDBIg93MAlLVsj9gABYg8QsFt7AFjLPM+L3/v28vShn5+madrv91e9H+7JyR4gwMkeFvr5+/h0/7rKHHALJ3tY4DT00/T8/LbCJHAbsQcIEHuAALGHBb5/PX0+f+41eFTzg6yDPcQQtFy7OvlRD/JZY1vu9kfqZA8QIPYAAWIPECD2AAFiDxAg9gABYg8QIPYAAWIPEOBfHJPlG62UONkDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwTs1h7gYF57AICROdkDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AME/AHsND4vD5/QFgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for task in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    img.set_data(env.render(mode='rgb_array'))\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        print('Score:', task+1)\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define the Architecture of the Policy\n",
    "We will define a neural network that encodes a stochastic policy.\n",
    "\n",
    "The network that we'll define takes the  envrironment's **state as input** and returns the probability that the agent should select each possible action as the output. The output will be two numbers say [0.8, 0.2]. This means that the agent will pushe the pole to the ledt with 80% probability. Afte selecting this action, it sends the action to the environment and receives the next state and a reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we initialize the NN, all weights are random. The goal of the agent is to figure out the appropriate weights in the NN, so that for each state, the NN always outputs probabilities that help the agent get a high score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "action space: Discrete(2)\n",
      "observation space: Box(4,)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(\"action space:\", env.action_space)\n",
    "print(\"observation space:\", env.observation_space)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size=4, h_size=16, a_size=2):\n",
    "        \"\"\"\n",
    "        Neural network that encodes the policy\n",
    "        \n",
    "        Params\n",
    "        s_size(int): dimension of each state (size of input layer)\n",
    "        h_size(int): size of hidden layer\n",
    "        a_size(int): number of potential actions (size of output layer)\n",
    "        \"\"\"\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # activation functions is ReLu (Rectified Linear unit)\n",
    "        x = F.relu(self.fcl(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Train the agent\n",
    "We'll use the REINFORCE algorithm(Monte Carlo Policy Gradients) to help the agent train the weights of the neural network while it plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
